{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace part of the OUTPUT_PATH to create a new folder \n",
    "# for the detection results\n",
    "\n",
    "OUTPUT_PATH = \"Temp/LoanApplications_Offline/\"\n",
    "NEW_OUTPUT_PATH = \"Temp/LoanApplications_Offline__DETECTION/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# My packages\n",
    "from source import parse_mxml as pm\n",
    "from source import log_representation as lr\n",
    "from source import plots as plts\n",
    "from source import drift_detection as dd\n",
    "from source import drift_localization as dl\n",
    "from source import offline_streaming_clustering as off_sc\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "import hdbscan\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from sklearn.base import clone as sk_clone \n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insensitive_glob(pattern):\n",
    "    def either(c):\n",
    "        return '[%s%s]' % (c.lower(), c.upper()) if c.isalpha() else c\n",
    "    return glob.glob(''.join(map(either, pattern)))\n",
    "\n",
    "def if_any(string, lista):\n",
    "    # If the string contains any of the values\n",
    "    # from the list 'lista'\n",
    "    for l in lista:\n",
    "        if l in string:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List log files\n",
    "logs = insensitive_glob(r\"../../../../../../../Datasets/Business_Process_Drift_Logs/Logs/*/*k.MXML\")\n",
    "logs = [x.replace('\\\\', '/') for x in logs if \"2.5\" not in x]\n",
    "# logs = [x for x in logs if \"2.5\" not in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference objects and map them to strings in dict \n",
    "# used in further methods\n",
    "objects = {\n",
    "    \"model\": {\n",
    "#         \"kmeans__k=6\": KMeans(n_clusters=6, random_state=42),\n",
    "#         \"kmeans__k=3\": KMeans(n_clusters=3, random_state=42),\n",
    "#         \"kmeans__k=2\": KMeans(n_clusters=2, random_state=42),\n",
    "#         \"DBSCAN__eps=05ms=5\": DBSCAN(eps=0.5, min_samples=5, metric='euclidean'),\n",
    "#         \"DBSCAN__eps=1ms=4\": DBSCAN(eps=1, min_samples=4, metric='euclidean'),\n",
    "#         \"DBSCAN__eps=2ms=3\": DBSCAN(eps=2, min_samples=3, metric='euclidean'),\n",
    "        \"HDBSCAN__noparams\": hdbscan.HDBSCAN(gen_min_span_tree=True, allow_single_cluster=True)\n",
    "    },\n",
    "    \n",
    "    \"representation\": {\n",
    "        \"activity_binary\": lr.get_binary_representation,\n",
    "        \"activity_frequency\": lr.get_frequency_representation,\n",
    "        \n",
    "        \"transitions_binary\": lr.get_binary_transitions_representation,\n",
    "        \"transitions_frequency\": lr.get_frequency_transitions_representation,\n",
    "        \n",
    "        \"activity_tfidf\": lr.get_tfidf_representation,\n",
    "        \"transitions_tfidf\": lr.get_tfidf_transitions_representation,\n",
    "        \n",
    "        \"activity_transitions_frequency\": lr.get_activity_transitions_frequency_representation,\n",
    "        \"activity_transitions_binary\": lr.get_activity_transitions_binary_representation\n",
    "    }\n",
    "#     \"representation\": {\n",
    "#         \"activity_binary\": lambda x: lr.get_binary_representation(lr.get_traces_as_tokens(x)),\n",
    "#         \"activity_frequency\": lambda x: lr.get_frequency_representation(lr.get_traces_as_tokens(x)),\n",
    "#         \"transitions_binary\": lambda x: lr.get_binary_transitions_representation(lr.get_traces_as_tokens(x)),\n",
    "#         \"transitions_frequency\": lambda x: lr.get_frequency_transitions_representation(lr.get_traces_as_tokens(x)),\n",
    "#         \"activity_transitions_frequency\": lambda x: pd.concat([lr.get_frequency_transitions_representation(lr.get_traces_as_tokens(x)), lr.get_frequency_representation(lr.get_traces_as_tokens(x))],axis=1),\n",
    "#         \"activity_transitions_binary\": lambda x: pd.concat([lr.get_binary_transitions_representation(lr.get_traces_as_tokens(x)), lr.get_binary_representation(lr.get_traces_as_tokens(x))],axis=1)\n",
    "#     }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change patterns and they supported representations\n",
    "activity_binary_drifts = [\"cb\", \"cf\", \"cm\", \"fr\", \"pm\", \"re\", \"rp\"]\n",
    "activity_frequency_drifts = activity_binary_drifts + [\"cp\", \"lp\"]\n",
    "\n",
    "transitions_binary_drifts = activity_frequency_drifts + [\"cd\", \"pl\", \"sw\"]\n",
    "transitions_frequency_drifts = transitions_binary_drifts\n",
    "\n",
    "activity_tfidf_drifts = transitions_binary_drifts\n",
    "transitions_tfidf_drifts = transitions_binary_drifts\n",
    "\n",
    "activity_transitions_frequency_drifts = transitions_binary_drifts\n",
    "activity_transitions_binary_drifts = transitions_binary_drifts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Offline Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_and_run_clustering_pipeline(args, return_result=False):\n",
    "    \"\"\"\n",
    "    Read an event log file, represent it into a feature vector space and\n",
    "    run the trace clustering method over windows. This method outputs results\n",
    "    as gzip csv files into the \"OUTPUT_PATH\" folder, or return the result \n",
    "    as DataFrame when return_result = True.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "        args (dict): Dictionary with the parameters and the log_file path\n",
    "            requiring the following keys:\n",
    "                example = {\n",
    "                    'log': <PATH TO LOG_FILE>,\n",
    "                    'representation': <KEY TO REPRESENTATIONS IN 'objects'>,\n",
    "                    'parameters': [{\n",
    "                        'model': <KEY TO MODEL IN 'objects'>, \n",
    "                        'sliding_window': <WHETHER TO USE SLIDING WINDOW>,\n",
    "                        'window_size': <SIZE OF TRACE WINDOW TO USE>,\n",
    "                        'sliding_step': <STEP OF SLIDING WINDOW>\n",
    "                    }\n",
    "        return_result (bool): Whether to return the result as DataFrame\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    # Treat file name to structure size and log type\n",
    "    split = args[\"log\"].split(\"/\")\n",
    "    \n",
    "    # Parse change pattern name\n",
    "    cd_name = split[-2]\n",
    "    log_name = split[-1][:-5]\n",
    "\n",
    "    # Parse size of the event_log\n",
    "    log_size = log_name.replace(cd_name, \"\").replace(\"new_\", \"\")\n",
    "    log_size = int(float(log_size.replace(\"k\", \"\")) * 1000)\n",
    "    \n",
    "    # Set up true drifts indexes\n",
    "#     y_true = [x for x in range(int(log_size/10), log_size, int(log_size/10))]\n",
    "    \n",
    "    try:\n",
    "        # Read log and apply trace representation technique\n",
    "        log_read = pm.all_prep(open(args[\"log\"]))\n",
    "        tokens = lr.get_traces_as_tokens(log_read)\n",
    "#         df = objects[\"representation\"][args[\"representation\"]](log_read)\n",
    "        \n",
    "        for p in args[\"parameters\"]:\n",
    "            # String to identify results when exporting files\n",
    "            tipo_mudanca = cd_name.replace(\"new_\", \"\")\n",
    "            \n",
    "            cached_info = \"_\".join([\n",
    "                tipo_mudanca,\n",
    "                str(log_size),\n",
    "                p[\"model\"],\n",
    "                args[\"representation\"],\n",
    "                str(p[\"window_size\"]),\n",
    "                str(p[\"sliding_window\"])\n",
    "            ])\n",
    "            \n",
    "#             print(cached_info)\n",
    "            \n",
    "            # If already exists, return if needed\n",
    "            file_to_export = OUTPUT_PATH + tipo_mudanca + '/' + cached_info + '.pickle.gzip'\n",
    "            if os.path.exists(file_to_export):\n",
    "                if return_result:\n",
    "                    r_ = pd.read_pickle(\n",
    "                        file_to_export,\n",
    "                        compression='gzip'\n",
    "                    )\n",
    "                    return r_\n",
    "                else:\n",
    "                    continue\n",
    "            \n",
    "            # If file does not exists, run trace clustering step and export file\n",
    "            all_metrics = off_sc.run_offline_clustering_window(\n",
    "                tokens,\n",
    "                objects[\"representation\"][args[\"representation\"]],\n",
    "                sk_clone(objects[\"model\"][p[\"model\"]]),\n",
    "                p[\"window_size\"],\n",
    "#                 df,\n",
    "                p[\"sliding_window\"],\n",
    "                sliding_step=p['sliding_step']\n",
    "            )\n",
    "            \n",
    "            # Set up true drifts indexes and append\n",
    "            y_true = list(range(int(len(tokens)/10), len(tokens), int(len(tokens)/10)))\n",
    "            all_metrics[\"y_true\"] = all_metrics.apply(lambda x: y_true, axis = 1)\n",
    "            \n",
    "            if return_result:\n",
    "                return all_metrics\n",
    "            else:\n",
    "                try:\n",
    "                    os.makedirs(OUTPUT_PATH + tipo_mudanca + '/')\n",
    "                except:\n",
    "                    pass\n",
    "                all_metrics.to_pickle(file_to_export, compression=\"gzip\")\n",
    "\n",
    "            gc.collect()\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run pipeline for specific case(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read_file_and_run_clustering_pipeline({\n",
    "#     'log': logs[0],\n",
    "#     'representation': 'activity_binary',\n",
    "#     'parameters': [{\n",
    "#         'model': 'HDBSCAN__noparams', \n",
    "#         'sliding_window': False,\n",
    "#         'window_size': 75,\n",
    "#         'sliding_step': 1\n",
    "#     }]\n",
    "# }, return_result=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Experiments with several parameters combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace clustering parameters\n",
    "grid_parameters = list(ParameterGrid({\n",
    "    \"sliding_window\": [False],    \n",
    "    \"window_size\": [100, 125, 150, 200],\n",
    "    'sliding_step': [1],\n",
    "    \"model\": [\n",
    "#         'kmeans__k=6',\n",
    "#         'kmeans__k=3',\n",
    "#         'kmeans__k=2',\n",
    "#         \"DBSCAN__eps=05ms=5\",\n",
    "#         \"DBSCAN__eps=1ms=4\",\n",
    "#         \"DBSCAN__eps=2ms=3\",\n",
    "        \"HDBSCAN__noparams\"\n",
    "    ] \n",
    "}))\n",
    "\n",
    "# Trace vector representations\n",
    "grid_logs = list(ParameterGrid([\n",
    "    { \"log\": [x for x in logs if if_any(x, activity_binary_drifts)],\n",
    "        \"representation\": [\"activity_binary\"]},\n",
    "    {\"log\": [x for x in logs if if_any(x, activity_frequency_drifts)],\n",
    "        \"representation\": [\"activity_frequency\"]},\n",
    "    \n",
    "    { \"log\": [x for x in logs if if_any(x, transitions_binary_drifts)],\n",
    "        \"representation\": [\"transitions_binary\"]},\n",
    "    \n",
    "    { \"log\": [x for x in logs if if_any(x, transitions_frequency_drifts)],\n",
    "        \"representation\": [\"transitions_frequency\"]},\n",
    "    \n",
    "    { \"log\": [x for x in logs if if_any(x, activity_tfidf_drifts)],\n",
    "        \"representation\": [\"activity_tfidf\"]},\n",
    "    {\"log\": [x for x in logs if if_any(x, transitions_tfidf_drifts)],\n",
    "        \"representation\": [\"activity_transitions_binary\"]},\n",
    "    \n",
    "    {\"log\": [x for x in logs if if_any(x, activity_transitions_frequency_drifts)],\n",
    "        \"representation\": [\"activity_transitions_frequency\"]},\n",
    "    {\"log\": [x for x in logs if if_any(x, activity_transitions_binary_drifts)],\n",
    "        \"representation\": [\"activity_transitions_binary\"]}\n",
    "]))\n",
    "\n",
    "# Combining all parameters\n",
    "combs = []\n",
    "for x in grid_logs:\n",
    "    dic = x.copy()\n",
    "    dic['parameters'] = grid_parameters\n",
    "    \n",
    "    combs.append(dic)\n",
    "\n",
    "len(combs), len(grid_parameters), len(combs) * len(grid_parameters) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run parallely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_resp = Parallel(n_jobs=-1)(\n",
    "    delayed(read_file_and_run_clustering_pipeline)(comb) for comb in tqdm_notebook(combs)\n",
    ")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drift detection parameters\n",
    "drift_config = list(ParameterGrid([\n",
    "    {\n",
    "        \"rolling_window\": [3, 4]#[3, 4, 5]\n",
    "        ,\"std_tolerance\": [3]#[1.25, 1.5, 1.75, 2]\n",
    "        ,'min_tol': [0.025]#[0.01, 0.007, 0.005, 0.003] \n",
    "    }\n",
    "]))\n",
    "print(len(drift_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files obtained after the trace clustering pipeline\n",
    "\n",
    "clusterizacoes = glob.glob(OUTPUT_PATH + \"*/*.pickle.gzip\")\n",
    "print(len(clusterizacoes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinations to run\n",
    "\n",
    "combs_deteccao = []\n",
    "for log in clusterizacoes:\n",
    "    combs_deteccao.append({\n",
    "        'input': log,\n",
    "        'combinations': drift_config\n",
    "    })\n",
    "print(len(combs_deteccao))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drift_detect_pipeline(args, return_results=False):\n",
    "    \"\"\"\n",
    "        Runs the drift detection method based on the output from the trace\n",
    "        clustering pipeline for different combination of parameters and every\n",
    "        feature from tracking the trace clustering evolution. The\n",
    "        outputs are into a new folder named by the NEW_OUTPUT_PATH variable in \n",
    "        gzip csv files.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "            args (dict): Dictionary with the parameters and the log_file path\n",
    "                requiring the following keys:\n",
    "                    example = {\n",
    "                    }\n",
    "    \"\"\"\n",
    "    # Read file\n",
    "    all_metrics = pd.read_pickle(args[\"input\"], compression='gzip')\n",
    "    \n",
    "    # Parse information from file name\n",
    "    path_file = args[\"input\"].replace(\".pickle.gzip\", \"\").split('\\\\')\n",
    "    args.update({\n",
    "        \"tipo_mudanca\": path_file[-1].split('_')[0],\n",
    "        \"log_size\": int(path_file[-1].split('_')[1]),\n",
    "        \"model\": \"_\".join(path_file[-1].split('_')[2:5]),\n",
    "        \"representation\": \"_\".join(path_file[-1].split('_')[5:-2]),\n",
    "        \"window_size\": path_file[-1].split('_')[-2],\n",
    "        \"sliding_window\": path_file[-1].split('_')[-1]\n",
    "    })\n",
    "    \n",
    "    # Run detection for every combination of parameter    \n",
    "    for combination in args['combinations']:\n",
    "        c = deepcopy(combination)\n",
    "        c.update({\n",
    "            'input': args['input'],\n",
    "            'tipo_mudanca': args['tipo_mudanca'],\n",
    "            'log_size': args['log_size'],\n",
    "            'model': args['model'],\n",
    "            'representation': args['representation'],\n",
    "            'window_size': args['window_size'],\n",
    "            'sliding_window': args['sliding_window']\n",
    "        })\n",
    "        \n",
    "        if return_results:\n",
    "            return __drift_detect_pipeline(\n",
    "                all_metrics, c, return_results\n",
    "            )\n",
    "        else:\n",
    "            __drift_detect_pipeline(\n",
    "                all_metrics, c, return_results\n",
    "            )\n",
    "    \n",
    "\n",
    "def __drift_detect_pipeline(all_metrics, args, return_results=False):     \n",
    "    base_name = args[\"input\"].replace(\".pickle.gzip\", \"\").replace(\"\\\\\", \"/\")\n",
    "    base_name = base_name.replace(OUTPUT_PATH, NEW_OUTPUT_PATH)\n",
    "    \n",
    "    # Create string with parameters to identify file\n",
    "    to_string = [\n",
    "        str(args[\"rolling_window\"]),\n",
    "        str(args[\"std_tolerance\"]).replace(\".\", \"-\"), \n",
    "        str(args[\"min_tol\"]).replace(\".\", \"-\")\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(base_name)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    final_name = base_name + \"/\" + \"_\".join(to_string) + \".pickle.gzip\"\n",
    "#     final_name = base_name.replace(\"\\\\\", \"/\") + \"_\".join(to_string) + \".pickle.gzip\"\n",
    "    \n",
    "    if os.path.isfile(final_name):\n",
    "        if return_results:\n",
    "            return pd.read_pickle(final_name, compression='gzip')\n",
    "        else:\n",
    "            print(\"Already exists\")\n",
    "            return\n",
    "    \n",
    "#     y_true = [x for x in range(int(args['log_size']/10), args['log_size'], int(args['log_size']/10))]\n",
    "    \n",
    "    # Runs the drift detection for every feature\n",
    "    results = []\n",
    "    for col in all_metrics.select_dtypes(include=np.number).columns:\n",
    "        if (col not in [\"i\",\"test_id\"]):\n",
    "#         if (col not in [\"k\"] and not col.startswith(\"diff\") ) or col in [\"diff_centroids\"]:\n",
    "            r = deepcopy(args)\n",
    "            r[\"measure\"] = col\n",
    "\n",
    "            detected_drifts, not_drifts, info = dd.detect_concept_drift(\n",
    "                all_metrics, \n",
    "                col,\n",
    "                args[\"rolling_window\"],\n",
    "                args[\"std_tolerance\"],\n",
    "                args[\"min_tol\"]\n",
    "            )    \n",
    "    \n",
    "            # Calculate classification metrics\n",
    "            metrics_results = dd.get_metrics(\n",
    "                detected_drifts,\n",
    "                not_drifts,\n",
    "                all_metrics[\"y_true\"].iloc[0], #y_true,\n",
    "                int(args[\"window_size\"])\n",
    "            )\n",
    "\n",
    "            r.update(args)\n",
    "            r.update(metrics_results)\n",
    "\n",
    "            results.append(r)\n",
    "\n",
    "            gc.collect()\n",
    "    \n",
    "    # Export as file\n",
    "    pd.DataFrame(results).to_pickle(\n",
    "        final_name,\n",
    "        compression=\"gzip\"\n",
    "    )\n",
    "    \n",
    "    if return_results:\n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    # print(col, len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detection_results = drift_detect_pipeline({\n",
    "#     'input': '../LoanApplications_Offline\\\\cb\\\\cb_10000_kmeans__k=2_binary_100_False.pickle.gzip',\n",
    "#     'combinations': [{\n",
    "#        'min_tol': 0.01,\n",
    "#        'rolling_window': 3,\n",
    "#        'std_tolerance': 1.25\n",
    "#     }, {\n",
    "#        'min_tol': 0.02,\n",
    "#        'rolling_window': 3,\n",
    "#        'std_tolerance': 1.25\n",
    "#     }]\n",
    "# }, return_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run parallely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_resp = Parallel(n_jobs=-1)(\n",
    "    delayed(drift_detect_pipeline)(comb_d) for comb_d in tqdm_notebook(combs_deteccao)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "#     'kmeans__k=6',\n",
    "#     'kmeans__k=3',\n",
    "#     'kmeans__k=2',\n",
    "#     \"DBSCAN__eps=05_ms=5\",\n",
    "#     \"DBSCAN__eps=1_ms=4\",\n",
    "#     \"DBSCAN__eps=2_ms=3\",\n",
    "    \"HDBSCAN__noparams\"\n",
    "]\n",
    "\n",
    "# Function to Read results\n",
    "def consolidate_results(log):\n",
    "    return pd.read_pickle(log, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    # List all files obtained after the trace clustering pipeline\n",
    "    deteccoes = glob.glob(NEW_OUTPUT_PATH + \"/*/*\"+model+\"*/*.pickle.gzip\")\n",
    "\n",
    "    # Call function to read results and then consolidate\n",
    "    all_results = pd.DataFrame()\n",
    "    all_results = all_results.append(Parallel(n_jobs=-1)(\n",
    "        delayed(consolidate_results)(log) for log in tqdm_notebook(deteccoes)\n",
    "    ))\n",
    "    \n",
    "    # Export as file\n",
    "    all_results.to_pickle(\n",
    "        'Temp/all_results_'+model,\n",
    "        compression=\"gzip\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import as file\n",
    "all_results = pd.read_pickle('Temp/all_results_'+models[0], compression='gzip')\n",
    "print(all_results.shape)\n",
    "\n",
    "# Group results by all params and get the mean F1 and Delay among all scenarios\n",
    "# In other words, what combination of params got the best results in all scenarios\n",
    "\n",
    "params = [\"min_tol\", \"rolling_window\", \"std_tolerance\", \"window_size\", \"measure\", \"representation\"]\n",
    "scenarios = [\"tipo_mudanca\", \"log_size\"]\n",
    "validation_metrics = [\"F1\",\"Delay\"]#,\"Support\",\"Precision\",\"Recall\"]\n",
    "\n",
    "all_results_grouped = all_results.groupby(params)[validation_metrics].agg(['mean','std'])\n",
    "all_results_grouped.columns = all_results_grouped.columns.map('_'.join)\n",
    "# all_results_grouped = all_results_grouped[all_results_grouped['F1_count']>=len(activity_binary_drifts)]\n",
    "all_results_grouped.sort_values([\"F1_mean\",\"Delay_mean\"], ascending=[False,True], inplace=True)\n",
    "all_results_grouped.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_results_by_representation = all_results_grouped.reset_index().groupby(\"representation\").first().reset_index().sort_values([\"F1_mean\",\"Delay_mean\"], ascending=[False,True])\n",
    "best_results_by_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_result = pd.merge(left=all_results\n",
    "            , right=best_results_by_representation\n",
    "            , how='inner', on=params)\n",
    "results_table = best_result.pivot_table(values=[\"F1\",\"Delay\"]\n",
    "                                        , index=[\"tipo_mudanca\"]\n",
    "                                        , columns=[\"representation\"]\n",
    "                                        , aggfunc='mean')\\\n",
    "                                    .sort_index(axis='columns',level=[1,0], ascending=[True,False])\n",
    "\n",
    "results_table.to_excel('Temp/Results/results_table_'+models[0]+'.xlsx', sheet_name=models[0])\n",
    "results_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results_table['F1'].plot(kind='bar', figsize=(20,10), fontsize=20)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5), fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table_logsize = best_result.pivot_table(values=[\"F1\"]#,\"Delay\"]\n",
    "                                                , index=[\"tipo_mudanca\"]\n",
    "                                                , columns=[\"representation\"\n",
    "                                                , \"log_size\"]\n",
    "                                                , aggfunc='mean')\\\n",
    "                                    .sort_index(axis='columns',level=[1,0], ascending=[True,False])\n",
    "\n",
    "results_table_logsize.to_excel('Temp/Results/results_table_logsize'+models[0]+'.xlsx', sheet_name=models[0])\n",
    "results_table_logsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shsow_result = pd.merge(left=best_result\n",
    "            , right=best_results_by_representation.head(1)\n",
    "            , how='inner', on=params)\n",
    "shsow_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for row in range(0,len(shsow_result)):\n",
    "\n",
    "    shsow_result_log = [x for x in logs if shsow_result.iloc[row][\"tipo_mudanca\"] in x \n",
    "         and str(shsow_result.iloc[row][\"log_size\"]/1000) in \n",
    "                    str(float(x.split(\"/\")[-1][:-5].replace(\"k\", \"\").replace(x.split(\"/\")[-2],\"\")))]\n",
    "    \n",
    "    log_read = pm.all_prep(open(shsow_result_log[0]))\n",
    "    tokens = lr.get_traces_as_tokens(log_read)\n",
    "\n",
    "    run_df = off_sc.run_offline_clustering_window(\n",
    "        tokens,\n",
    "        objects[\"representation\"][shsow_result.iloc[row]['representation']],\n",
    "        objects[\"model\"][shsow_result.iloc[row]['model']],\n",
    "        int(shsow_result.iloc[row]['window_size']),\n",
    "        sliding_window=False,\n",
    "        sliding_step=1\n",
    "    )\n",
    "\n",
    "    drifts, not_drifts, info = dd.detect_concept_drift(\n",
    "        run_df,\n",
    "        shsow_result.iloc[row]['measure'],\n",
    "        rolling_window=shsow_result.iloc[row]['rolling_window'],\n",
    "        std_tolerance=shsow_result.iloc[row]['std_tolerance'],\n",
    "        min_tol=shsow_result.iloc[row]['min_tol']\n",
    "    )\n",
    "\n",
    "    plts.plot_deteccao_drift(\n",
    "        run_df,\n",
    "        shsow_result.iloc[row]['measure'],\n",
    "        shsow_result.iloc[row]['Drifts_Found'],\n",
    "        shsow_result.iloc[row]['Resp'],\n",
    "        info['means'],\n",
    "        info['lowers'],\n",
    "        info['uppers'],\n",
    "        save_png=\"\"\n",
    "    )\n",
    "    plt.title(\"Log: \" + shsow_result.iloc[row][\"tipo_mudanca\"] + str(shsow_result.iloc[row][\"log_size\"]) \n",
    "                  + \" - Rep: \" + shsow_result.iloc[row][\"representation\"] \n",
    "                  + \" - Metric: \" + shsow_result.iloc[row][\"measure\"]\n",
    "                  + \" - F1: \" + str(round(shsow_result.iloc[row][\"F1\"],2))\n",
    "              , size=30)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_results.to_excel('Resultados_PL.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
