{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace part of the OUTPUT_PATH to create a new folder \n",
    "# for the detection results\n",
    "\n",
    "OUTPUT_PATH = \"Temp/LoanApplications_Offline/\"\n",
    "NEW_OUTPUT_PATH = \"Temp/LoanApplications_Offline__DETECTION/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# My packages\n",
    "from source import parse_mxml as pm\n",
    "from source import log_representation as lr\n",
    "from source import plots as plts\n",
    "from source import drift_detection as dd\n",
    "from source import drift_localization as dl\n",
    "from source import offline_streaming_clustering as off_sc\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "import hdbscan\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from sklearn.base import clone as sk_clone \n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insensitive_glob(pattern):\n",
    "    def either(c):\n",
    "        return '[%s%s]' % (c.lower(), c.upper()) if c.isalpha() else c\n",
    "    return glob.glob(''.join(map(either, pattern)))\n",
    "\n",
    "def if_any(string, lista):\n",
    "    # If the string contains any of the values\n",
    "    # from the list 'lista'\n",
    "    for l in lista:\n",
    "        if l in string:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List log files\n",
    "logs = insensitive_glob(r\"../../../../../../../Datasets/Business_Process_Drift_Logs/Logs/*/*k.MXML\")\n",
    "logs = [x.replace('\\\\', '/') for x in logs if \"2.5\" not in x]\n",
    "# logs = [x for x in logs if \"2.5\" not in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference objects and map them to strings in dict \n",
    "# used in further methods\n",
    "objects = {\n",
    "    \"model\": {\n",
    "#         \"kmeans__k=6\": KMeans(n_clusters=6, random_state=42),\n",
    "#         \"kmeans__k=3\": KMeans(n_clusters=3, random_state=42),\n",
    "#         \"kmeans__k=2\": KMeans(n_clusters=2, random_state=42),\n",
    "#         \"DBSCAN__eps=05ms=5\": DBSCAN(eps=0.5, min_samples=5, metric='euclidean'),\n",
    "#         \"DBSCAN__eps=1ms=4\": DBSCAN(eps=1, min_samples=4, metric='euclidean'),\n",
    "#         \"DBSCAN__eps=2ms=3\": DBSCAN(eps=2, min_samples=3, metric='euclidean'),\n",
    "        \"HDBSCAN__noparams\": hdbscan.HDBSCAN(gen_min_span_tree=True, allow_single_cluster=True)\n",
    "    },\n",
    "    \n",
    "    \"representation\": {\n",
    "        \"activity_binary\": lr.get_binary_representation,\n",
    "        \"activity_frequency\": lr.get_frequency_representation,\n",
    "        \n",
    "        \"transitions_binary\": lr.get_binary_transitions_representation,\n",
    "        \"transitions_frequency\": lr.get_frequency_transitions_representation,\n",
    "        \n",
    "        \"activity_tfidf\": lr.get_tfidf_representation,\n",
    "        \"transitions_tfidf\": lr.get_tfidf_transitions_representation,\n",
    "        \n",
    "        \"activity_transitions_frequency\": lr.get_activity_transitions_frequency_representation,\n",
    "        \"activity_transitions_binary\": lr.get_activity_transitions_binary_representation\n",
    "    }\n",
    "#     \"representation\": {\n",
    "#         \"activity_binary\": lambda x: lr.get_binary_representation(lr.get_traces_as_tokens(x)),\n",
    "#         \"activity_frequency\": lambda x: lr.get_frequency_representation(lr.get_traces_as_tokens(x)),\n",
    "#         \"transitions_binary\": lambda x: lr.get_binary_transitions_representation(lr.get_traces_as_tokens(x)),\n",
    "#         \"transitions_frequency\": lambda x: lr.get_frequency_transitions_representation(lr.get_traces_as_tokens(x)),\n",
    "#         \"activity_transitions_frequency\": lambda x: pd.concat([lr.get_frequency_transitions_representation(lr.get_traces_as_tokens(x)), lr.get_frequency_representation(lr.get_traces_as_tokens(x))],axis=1),\n",
    "#         \"activity_transitions_binary\": lambda x: pd.concat([lr.get_binary_transitions_representation(lr.get_traces_as_tokens(x)), lr.get_binary_representation(lr.get_traces_as_tokens(x))],axis=1)\n",
    "#     }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change patterns and they supported representations\n",
    "activity_binary_drifts = [\"cb\", \"cf\", \"cm\", \"fr\", \"pm\", \"re\", \"rp\"]\n",
    "activity_frequency_drifts = activity_binary_drifts + [\"cp\", \"lp\"]\n",
    "\n",
    "transitions_binary_drifts = activity_frequency_drifts + [\"cd\", \"pl\", \"sw\"]\n",
    "transitions_frequency_drifts = transitions_binary_drifts\n",
    "\n",
    "activity_tfidf_drifts = transitions_binary_drifts\n",
    "transitions_tfidf_drifts = transitions_binary_drifts\n",
    "\n",
    "activity_transitions_frequency_drifts = transitions_binary_drifts\n",
    "activity_transitions_binary_drifts = transitions_binary_drifts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Offline Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_and_run_clustering_pipeline(args, return_result=False):\n",
    "    \"\"\"\n",
    "    Read an event log file, represent it into a feature vector space and\n",
    "    run the trace clustering method over windows. This method outputs results\n",
    "    as gzip csv files into the \"OUTPUT_PATH\" folder, or return the result \n",
    "    as DataFrame when return_result = True.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "        args (dict): Dictionary with the parameters and the log_file path\n",
    "            requiring the following keys:\n",
    "                example = {\n",
    "                    'log': <PATH TO LOG_FILE>,\n",
    "                    'representation': <KEY TO REPRESENTATIONS IN 'objects'>,\n",
    "                    'parameters': [{\n",
    "                        'model': <KEY TO MODEL IN 'objects'>, \n",
    "                        'sliding_window': <WHETHER TO USE SLIDING WINDOW>,\n",
    "                        'window_size': <SIZE OF TRACE WINDOW TO USE>,\n",
    "                        'sliding_step': <STEP OF SLIDING WINDOW>\n",
    "                    }\n",
    "        return_result (bool): Whether to return the result as DataFrame\n",
    "            \n",
    "    \"\"\"\n",
    "    # Create final dataset\n",
    "    all_results = pd.DataFrame()\n",
    "    \n",
    "    # Treat file name to structure size and log type\n",
    "    split = args[\"log\"].split(\"/\")\n",
    "    \n",
    "    # Parse change pattern name\n",
    "    tipo_mudanca = split[-2]\n",
    "    log_name = split[-1][:-5]\n",
    "\n",
    "    # Parse size of the event_log\n",
    "    log_size = int(float(log_name.replace(tipo_mudanca, \"\").replace(\"k\", \"\")) * 1000)\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Read log and apply trace representation technique\n",
    "        log_read = pm.all_prep(open(args[\"log\"]))\n",
    "        tokens = lr.get_traces_as_tokens(log_read)\n",
    "#         df = objects[\"representation\"][args[\"representation\"]](log_read)\n",
    "        \n",
    "        # Create metric dataset\n",
    "        all_metrics = pd.DataFrame() \n",
    "        \n",
    "        for p in args[\"parameters\"]:\n",
    "            print(p)\n",
    "            \n",
    "            # If file does not exists, run trace clustering step and export file\n",
    "            all_metrics = off_sc.run_offline_clustering_window(\n",
    "                tokens,\n",
    "                objects[\"representation\"][args[\"representation\"]],\n",
    "                sk_clone(objects[\"model\"][p[\"model\"]]),\n",
    "                p[\"window_size\"],\n",
    "#                 df,\n",
    "                p[\"sliding_window\"],\n",
    "                sliding_step=p['sliding_step']\n",
    "            )\n",
    "            \n",
    "            # Set up true drifts indexes and append\n",
    "            y_true = list(range(int(len(tokens)/10), len(tokens), int(len(tokens)/10)))\n",
    "            all_metrics[\"y_true\"] = all_metrics.apply(lambda x: y_true, axis = 1)\n",
    "            \n",
    "            all_metrics = all_metrics.reset_index()\n",
    "            \n",
    "            # Identify columns\n",
    "            all_metrics['tipo_mudanca'] = tipo_mudanca\n",
    "            all_metrics['log_size'] = str(log_size)\n",
    "            all_metrics['model'] = p[\"model\"]\n",
    "            all_metrics['representation'] = args[\"representation\"]\n",
    "            all_metrics['window_size'] = str(p[\"window_size\"])\n",
    "            all_metrics['sliding_window'] = str(p[\"sliding_window\"])\n",
    "            \n",
    "        \n",
    "            # Append results in final dataset\n",
    "            all_results = all_results.append(all_metrics)\n",
    "            \n",
    "            gc.collect()\n",
    "            \n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    \n",
    "    all_results = all_results.reset_index(drop=True)\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run pipeline for specific case(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read_file_and_run_clustering_pipeline({\n",
    "#     'log': logs[0],\n",
    "#     'representation': 'activity_binary',\n",
    "#     'parameters': [{\n",
    "#         'model': 'DBSCAN__eps=05ms=5', \n",
    "#         'sliding_window': False,\n",
    "#         'window_size': 150,\n",
    "#         'sliding_step': 1\n",
    "#     }]\n",
    "# }, return_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read_file_and_run_clustering_pipeline({\n",
    "#     'log': logs[0],\n",
    "#     'representation': 'activity_binary',\n",
    "#     'parameters': [{\n",
    "#         'model': 'kmeans__k=6', \n",
    "#         'sliding_window': False,\n",
    "#         'window_size': 200,\n",
    "#         'sliding_step': 1\n",
    "#     }]\n",
    "# }, return_result=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Experiments with several parameters combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace clustering parameters\n",
    "grid_parameters = list(ParameterGrid({\n",
    "    \"sliding_window\": [False]    \n",
    "    ,\"window_size\": [75, 100, 125, 150, 200]\n",
    "    ,\"sliding_step\": [1]\n",
    "    ,\"model\": [\n",
    "#         'kmeans__k=6',\n",
    "#         'kmeans__k=3',\n",
    "#         'kmeans__k=2',\n",
    "#         \"DBSCAN__eps=05ms=5\",\n",
    "#         \"DBSCAN__eps=1ms=4\",\n",
    "#         \"DBSCAN__eps=2ms=3\",\n",
    "        \"HDBSCAN__noparams\"\n",
    "    ] \n",
    "}))\n",
    "\n",
    "# Trace vector representations\n",
    "grid_logs = list(ParameterGrid([\n",
    "    { \"log\": [x for x in logs if if_any(x, activity_binary_drifts)],\n",
    "        \"representation\": [\"activity_binary\"]},\n",
    "    {\"log\": [x for x in logs if if_any(x, activity_frequency_drifts)],\n",
    "        \"representation\": [\"activity_frequency\"]},\n",
    "    \n",
    "    { \"log\": [x for x in logs if if_any(x, transitions_binary_drifts)],\n",
    "        \"representation\": [\"transitions_binary\"]},\n",
    "    \n",
    "    { \"log\": [x for x in logs if if_any(x, transitions_frequency_drifts)],\n",
    "        \"representation\": [\"transitions_frequency\"]},\n",
    "    \n",
    "    { \"log\": [x for x in logs if if_any(x, activity_tfidf_drifts)],\n",
    "        \"representation\": [\"activity_tfidf\"]},\n",
    "    {\"log\": [x for x in logs if if_any(x, transitions_tfidf_drifts)],\n",
    "        \"representation\": [\"activity_transitions_binary\"]},\n",
    "    \n",
    "    {\"log\": [x for x in logs if if_any(x, activity_transitions_frequency_drifts)],\n",
    "        \"representation\": [\"activity_transitions_frequency\"]},\n",
    "    {\"log\": [x for x in logs if if_any(x, activity_transitions_binary_drifts)],\n",
    "        \"representation\": [\"activity_transitions_binary\"]}\n",
    "]))\n",
    "\n",
    "# Combining all parameters\n",
    "combs = []\n",
    "for x in grid_logs:\n",
    "    dic = x.copy()\n",
    "    dic['parameters'] = grid_parameters \n",
    "    \n",
    "    combs.append(dic)\n",
    "\n",
    "len(combs), len(grid_parameters), len(combs) * len(grid_parameters) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run parallely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_resp = pd.DataFrame()\n",
    "final_resp = final_resp.append(Parallel(n_jobs=-2)(\n",
    "    delayed(read_file_and_run_clustering_pipeline)(comb,return_result=False) for comb in tqdm_notebook(combs)\n",
    "))\n",
    "\n",
    "final_resp = final_resp.reset_index(drop=True)\n",
    "\n",
    "try:\n",
    "    os.makedirs(OUTPUT_PATH)\n",
    "except:\n",
    "    pass\n",
    "final_resp.to_pickle(OUTPUT_PATH + \"clustering_results_HDBSCAN__noparams\" + '.pickle.gzip', compression=\"gzip\")\n",
    "\n",
    "final_resp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drift detection parameters\n",
    "drift_config = list(ParameterGrid([\n",
    "    {\n",
    "        \"rolling_window\": [3, 4]\n",
    "        ,\"std_tolerance\": [2, 2.5, 3]\n",
    "        ,\"min_tol\": [0.01 ,0.025, 0.05] #0.0025, 0.005,\n",
    "    }\n",
    "]))\n",
    "len(drift_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drift_detect_pipeline(drift_comb, return_results=False):\n",
    "    # Runs the drift detection for every feature\n",
    "    results = []\n",
    "    for col in drift_comb.select_dtypes(include=np.number).columns:\n",
    "        if (col not in [\"i\",\"test_id\"]):\n",
    "#         if (col in [\"Silhouette\"]):\n",
    "#         if (col not in [\"k\"] and not col.startswith(\"diff\") ) or col in [\"diff_centroids\"]:\n",
    "\n",
    "            drift_comb_temp = drift_comb.copy()\n",
    "            drift_comb_temp[\"measure\"] = col\n",
    "            \n",
    "            # Define index as iteration number (i)\n",
    "            drift_comb_temp.set_index(\"i\", inplace=True)\n",
    "    \n",
    "            detected_drifts, not_drifts, info = dd.detect_concept_drift(\n",
    "                drift_comb_temp, \n",
    "                col,\n",
    "                int(drift_comb_temp[\"rolling_window\"].iloc[0]),\n",
    "                float(drift_comb_temp[\"std_tolerance\"].iloc[0]),\n",
    "                float(drift_comb_temp[\"min_tol\"].iloc[0])\n",
    "            )    \n",
    "            \n",
    "            # Calculate classification metrics\n",
    "            metrics_results = dd.get_metrics(\n",
    "                detected_drifts,\n",
    "                not_drifts,\n",
    "                drift_comb_temp[\"y_true\"].iloc[0],\n",
    "                int(drift_comb_temp[\"window_size\"].iloc[0])\n",
    "            )\n",
    "            \n",
    "            drift_comb_temp.reset_index(inplace=True)\n",
    "            info = pd.DataFrame(info, index=drift_comb_temp.index)\n",
    "            metrics_results = pd.DataFrame([metrics_results], index=drift_comb_temp.index)\n",
    "\n",
    "            results = pd.concat([drift_comb_temp,metrics_results,info], axis=1)\n",
    "\n",
    "        gc.collect()\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run parallely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file\n",
    "clusterings = pd.read_pickle(OUTPUT_PATH + \"clustering_results_HDBSCAN__noparams.pickle.gzip\", compression='gzip')\n",
    "clusterings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combining all parameters\n",
    "drifts_combinations = pd.DataFrame()\n",
    "for config in tqdm_notebook(drift_config):\n",
    "    temp = clusterings.copy()\n",
    "    temp[\"min_tol\"] = str(config[\"min_tol\"])\n",
    "    temp[\"rolling_window\"] = str(config[\"rolling_window\"])\n",
    "    temp[\"std_tolerance\"] = str(config[\"std_tolerance\"])\n",
    "    drifts_combinations = drifts_combinations.append(temp)\n",
    "\n",
    "drifts_combinations = drifts_combinations.reset_index(drop=True)\n",
    "drifts_combinations[\"test_id\"] = drifts_combinations.groupby([\n",
    "    'tipo_mudanca','log_size','model','representation','window_size','sliding_window'\n",
    "    ,'min_tol','rolling_window','std_tolerance'\n",
    "    ]).ngroup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "drifts_combinations_grouped = drifts_combinations.groupby(\"test_id\")\n",
    "final_drift_detection = pd.DataFrame()\n",
    "final_drift_detection = final_drift_detection.append(Parallel(n_jobs=-1)(\n",
    "    delayed(drift_detect_pipeline)(drift_comb) for i,drift_comb in tqdm_notebook(drifts_combinations_grouped)\n",
    "))\n",
    "\n",
    "final_drift_detection = final_drift_detection.reset_index(drop=True)\n",
    "\n",
    "try:\n",
    "    os.makedirs(NEW_OUTPUT_PATH)\n",
    "except:\n",
    "    pass\n",
    "final_drift_detection.to_pickle(NEW_OUTPUT_PATH + \"drift_detections_results_HDBSCAN__noparams\" + '.pickle.gzip', compression=\"gzip\")\n",
    "\n",
    "\n",
    "final_drift_detection.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run pipeline for specific case(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "drift_detect_pipeline(drifts_combinations[drifts_combinations[\"test_id\"]==0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolidate results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file\n",
    "detections = pd.read_pickle(NEW_OUTPUT_PATH + \"drift_detections_results.pickle.gzip\", compression='gzip')\n",
    "detections.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = list(objects[\"model\"].keys())[1]\n",
    "print(model)\n",
    "\n",
    "# Filter tests\n",
    "detections_filtered = detections[detections[\"model\"]==model]\n",
    "print(detections_filtered.shape)\n",
    "\n",
    "all_results = final_drift_detection.set_index(\"test_id\").groupby(\"test_id\").first()\n",
    "\n",
    "# Group results by all params and get the mean F1 and Delay among all scenarios\n",
    "# In other words, what combination of params got the best results in all scenarios\n",
    "\n",
    "params = [\"min_tol\", \"rolling_window\", \"std_tolerance\", \"window_size\", \"measure\", \"representation\"]\n",
    "scenarios = [\"tipo_mudanca\", \"log_size\"]\n",
    "validation_metrics = [\"F1\",\"Delay\"]#,\"Support\",\"Precision\",\"Recall\"]\n",
    "\n",
    "all_results_grouped = all_results.groupby(params)[validation_metrics].agg(['mean','std'])\n",
    "all_results_grouped.columns = all_results_grouped.columns.map('_'.join)\n",
    "# all_results_grouped = all_results_grouped[all_results_grouped['F1_count']>=len(activity_binary_drifts)]\n",
    "all_results_grouped.sort_values([\"F1_mean\",\"Delay_mean\"], ascending=[False,True], inplace=True)\n",
    "all_results_grouped = all_results_grouped.reset_index()\n",
    "all_results_grouped.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_results_by_representation = all_results_grouped.reset_index().groupby(\"representation\").first().reset_index()\n",
    "best_results_by_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_result = pd.merge(left=all_results.reset_index()\n",
    "            , right=best_results_by_representation\n",
    "            , how='inner', on=params).set_index(\"test_id\")\n",
    "results_table = best_result.pivot_table(values=[\"F1\"]#,\"Delay\"]\n",
    "                                        , index=[\"tipo_mudanca\"]\n",
    "                                        , columns=[\"representation\"]\n",
    "                                        , aggfunc='mean')\\\n",
    "                                    .sort_index(axis='columns',level=[1,0], ascending=[True,False])\n",
    "results_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results_table['F1'].plot(kind='bar', figsize=(20,10), fontsize=20)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5), fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table_logsize = best_result.pivot_table(values=[\"F1\"]#,\"Delay\"]\n",
    "                                                , index=[\"tipo_mudanca\"]\n",
    "                                                , columns=[\"representation\", \"log_size\"]\n",
    "                                                , aggfunc='mean')\\\n",
    "                                    .sort_index(axis='columns',level=[1,0], ascending=[True,False])\n",
    "results_table_logsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for row in range(0,len(best_result)):\n",
    "    best_result_drifts = detections_filtered[detections_filtered[\"test_id\"]==best_result.index[row]]\n",
    "    best_result_drifts.set_index(\"i\", inplace=True)\n",
    "    \n",
    "    plts.plot_deteccao_drift(\n",
    "        best_result_drifts,\n",
    "        best_result_drifts.iloc[0]['measure'],\n",
    "        best_result_drifts.iloc[0]['Drifts_Found'],\n",
    "        best_result_drifts.iloc[0]['Resp'],\n",
    "        best_result_drifts['means'],\n",
    "        best_result_drifts['lowers'],\n",
    "        best_result_drifts['uppers'],\n",
    "        save_png=\"\"\n",
    "    )\n",
    "    plt.title(\"Log: \" + best_result_drifts.iloc[0][\"tipo_mudanca\"] + str(best_result_drifts.iloc[0][\"log_size\"]) \n",
    "                  + \" - Rep: \" + best_result_drifts.iloc[0][\"representation\"] \n",
    "                  + \" - Metric: \" + best_result_drifts.iloc[0][\"measure\"]\n",
    "                  + \" - F1: \" + str(round(best_result_drifts.iloc[0][\"F1\"],2))\n",
    "              , size=30)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for row in range(0,len(best_result)):\n",
    "\n",
    "#     best_result_log = [x for x in logs if best_result.iloc[row][\"tipo_mudanca\"] in x \n",
    "#          and str(best_result.iloc[row][\"log_size\"]/1000) in \n",
    "#                        str(float(x.split(\"/\")[-1][:-5].replace(\"k\", \"\").replace(x.split(\"/\")[-2],\"\")))]\n",
    "    \n",
    "#     run_df = off_sc.run_offline_clustering_window(\n",
    "#         objects[\"model\"][best_result.iloc[row]['model']],\n",
    "#         int(best_result.iloc[row]['window_size']),\n",
    "#         objects[\"representation\"][best_result.iloc[row]['representation']](pm.all_prep(open(best_result_log[0]))),\n",
    "#         sliding_window=False,\n",
    "#         sliding_step=1\n",
    "#     )\n",
    "\n",
    "#     drifts, info = dd.detect_concept_drift(\n",
    "#         run_df,\n",
    "#         best_result.iloc[row]['measure'],\n",
    "#         rolling_window=best_result.iloc[row]['rolling_window'],\n",
    "#         std_tolerance=best_result.iloc[row]['std_tolerance'],\n",
    "#         min_tol=best_result.iloc[row]['min_tol']\n",
    "#     )\n",
    "\n",
    "#     plts.plot_deteccao_drift(\n",
    "#         run_df,\n",
    "#         best_result.iloc[row]['measure'],\n",
    "#         best_result.iloc[row]['Drifts_Found'],\n",
    "#         best_result.iloc[row]['Resp'],\n",
    "#         info['means'],\n",
    "#         info['lowers'],\n",
    "#         info['uppers'],\n",
    "#         save_png=\"\"\n",
    "#     )\n",
    "#     plt.title(\"Log: \" + best_result.iloc[row][\"tipo_mudanca\"] + str(best_result.iloc[row][\"log_size\"]) \n",
    "#                   + \" - Rep: \" + best_result.iloc[row][\"representation\"] \n",
    "#                   + \" - Metric: \" + best_result.iloc[row][\"measure\"]\n",
    "#                   + \" - F1: \" + str(round(best_result.iloc[row][\"F1\"],2))\n",
    "#               , size=30)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_results.to_excel('Resultados_PL.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
